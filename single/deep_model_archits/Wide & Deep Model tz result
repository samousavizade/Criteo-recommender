digraph {
	graph [size="13.35,13.35"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140059306428192 [label="
 (2725)" fillcolor=darkolivegreen1]
	140059306448928 [label=SigmoidBackward0]
	140059306449024 -> 140059306448928
	140059306449024 [label=SqueezeBackward1]
	140059306448688 -> 140059306449024
	140059306448688 [label=AddBackward0]
	140059306449072 -> 140059306448688
	140059306449072 [label=AddBackward0]
	140059306449216 -> 140059306449072
	140059306449216 [label=SumBackward1]
	140059306449360 -> 140059306449216
	140059306449360 [label=EmbeddingBackward0]
	140059306449456 -> 140059306449360
	140059483695920 [label="linear.fc.weight
 (1944, 1)" fillcolor=lightblue]
	140059483695920 -> 140059306449456
	140059306449456 [label=AccumulateGrad]
	140059306449168 -> 140059306449072
	140059483696800 [label="linear.bias
 (1)" fillcolor=lightblue]
	140059483696800 -> 140059306449168
	140059306449168 [label=AccumulateGrad]
	140059306448832 -> 140059306448688
	140059306448832 [label=AddmmBackward0]
	140059306449408 -> 140059306448832
	140059363650512 [label="mlp.mlp.8.bias
 (1)" fillcolor=lightblue]
	140059363650512 -> 140059306449408
	140059306449408 [label=AccumulateGrad]
	140059306449552 -> 140059306448832
	140059306449552 [label=ReluBackward0]
	140059306449504 -> 140059306449552
	140059306449504 [label=NativeBatchNormBackward0]
	140059306449696 -> 140059306449504
	140059306449696 [label=AddmmBackward0]
	140059306449888 -> 140059306449696
	140059483701552 [label="mlp.mlp.4.bias
 (16)" fillcolor=lightblue]
	140059483701552 -> 140059306449888
	140059306449888 [label=AccumulateGrad]
	140059306449840 -> 140059306449696
	140059306449840 [label=ReluBackward0]
	140059306449984 -> 140059306449840
	140059306449984 [label=NativeBatchNormBackward0]
	140059306450176 -> 140059306449984
	140059306450176 [label=AddmmBackward0]
	140059306450368 -> 140059306450176
	140059483696640 [label="mlp.mlp.0.bias
 (16)" fillcolor=lightblue]
	140059483696640 -> 140059306450368
	140059306450368 [label=AccumulateGrad]
	140059306450320 -> 140059306450176
	140059306450320 [label=ViewBackward0]
	140059306450464 -> 140059306450320
	140059306450464 [label=EmbeddingBackward0]
	140059306450656 -> 140059306450464
	140059483695120 [label="embedding.embedding.weight
 (1944, 16)" fillcolor=lightblue]
	140059483695120 -> 140059306450656
	140059306450656 [label=AccumulateGrad]
	140059306450272 -> 140059306450176
	140059306450272 [label=TBackward0]
	140059306450752 -> 140059306450272
	140059483694400 [label="mlp.mlp.0.weight
 (16, 224)" fillcolor=lightblue]
	140059483694400 -> 140059306450752
	140059306450752 [label=AccumulateGrad]
	140059306450128 -> 140059306449984
	140059483688768 [label="mlp.mlp.1.weight
 (16)" fillcolor=lightblue]
	140059483688768 -> 140059306450128
	140059306450128 [label=AccumulateGrad]
	140059306450080 -> 140059306449984
	140059483687328 [label="mlp.mlp.1.bias
 (16)" fillcolor=lightblue]
	140059483687328 -> 140059306450080
	140059306450080 [label=AccumulateGrad]
	140059306449792 -> 140059306449696
	140059306449792 [label=TBackward0]
	140059306450416 -> 140059306449792
	140059483702512 [label="mlp.mlp.4.weight
 (16, 16)" fillcolor=lightblue]
	140059483702512 -> 140059306450416
	140059306450416 [label=AccumulateGrad]
	140059306449648 -> 140059306449504
	140059483701712 [label="mlp.mlp.5.weight
 (16)" fillcolor=lightblue]
	140059483701712 -> 140059306449648
	140059306449648 [label=AccumulateGrad]
	140059306449600 -> 140059306449504
	140059361029952 [label="mlp.mlp.5.bias
 (16)" fillcolor=lightblue]
	140059361029952 -> 140059306449600
	140059306449600 [label=AccumulateGrad]
	140059306449120 -> 140059306448832
	140059306449120 [label=TBackward0]
	140059306449936 -> 140059306449120
	140059361029792 [label="mlp.mlp.8.weight
 (1, 16)" fillcolor=lightblue]
	140059361029792 -> 140059306449936
	140059306449936 [label=AccumulateGrad]
	140059306448928 -> 140059306428192
}
