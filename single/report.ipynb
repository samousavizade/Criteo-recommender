{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sale</th>\n",
       "      <th>SalesAmountInEuro</th>\n",
       "      <th>time_delay_for_conversion</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>nb_clicks_1week</th>\n",
       "      <th>product_price</th>\n",
       "      <th>product_age_group</th>\n",
       "      <th>device_type</th>\n",
       "      <th>audience_id</th>\n",
       "      <th>product_gender</th>\n",
       "      <th>...</th>\n",
       "      <th>product_category(3)</th>\n",
       "      <th>product_category(4)</th>\n",
       "      <th>product_category(5)</th>\n",
       "      <th>product_category(6)</th>\n",
       "      <th>product_category(7)</th>\n",
       "      <th>product_country</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>partner_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-08-04 04:07:56</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7E56C27BFF0305E788DA55A029EC4988</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>57A1D462A03BD076E029CF9310C11FC5</td>\n",
       "      <td>B69E439E41E0BEAD764ABF16D7FD96C9</td>\n",
       "      <td>-1</td>\n",
       "      <td>E3DDEB04F8AFF944B11943BB57D2F620</td>\n",
       "      <td>5E2C678F6586B67F61A377E1534E01FC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-08-04 01:47:40</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>7E56C27BFF0305E788DA55A029EC4988</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>57A1D462A03BD076E029CF9310C11FC5</td>\n",
       "      <td>404D3D9D03297504F3509032DCFA02F0</td>\n",
       "      <td>-1</td>\n",
       "      <td>E3DDEB04F8AFF944B11943BB57D2F620</td>\n",
       "      <td>E8247702C0DD294E0AE6B5B5E2F9E810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-08-04 16:54:31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4C90FD52FC53D2C1C205844CB69575AB</td>\n",
       "      <td>FF2C446555E3822B0E0FC3406116E86D</td>\n",
       "      <td>-1</td>\n",
       "      <td>C45A9AC6D102ACAEEDF0D6F78636D84A</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2AC62132FBCFA093B9426894A4BC6278</td>\n",
       "      <td>B09E51338E0EED59C5A859B13631C370</td>\n",
       "      <td>3DF2BEDE6A8FDFA7F97B97FFF6EF38CC 516EE9C34B839...</td>\n",
       "      <td>12E43E51784BDE3CB9E0EF6310A7D5C5</td>\n",
       "      <td>9FD001258907F541D497040C64383696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-08-03 20:34:28</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>D7D1FB49049702BF6338894757E0D959</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>57A1D462A03BD076E029CF9310C11FC5</td>\n",
       "      <td>080614393A57816D7A655695E2DBE728</td>\n",
       "      <td>-1</td>\n",
       "      <td>E3DDEB04F8AFF944B11943BB57D2F620</td>\n",
       "      <td>225741ACF2DBB2E5948268F5D5D352E6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-08-03 20:54:17</td>\n",
       "      <td>1064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4C90FD52FC53D2C1C205844CB69575AB</td>\n",
       "      <td>7E56C27BFF0305E788DA55A029EC4988</td>\n",
       "      <td>-1</td>\n",
       "      <td>1B491180398E2F0390E6A588B3BCE291</td>\n",
       "      <td>...</td>\n",
       "      <td>1E629AECC2FB9BEF43331CBE8F2D7C08</td>\n",
       "      <td>341C9BD18A3277E6B104CAFC177DE796</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>989EEF92F2A525DD896557425EA7C4C7</td>\n",
       "      <td>350271BAFD7C7AAF6FB1424CF3DD4827</td>\n",
       "      <td>097C90F8BF5398AC7C486804F0801DEE E09E084DB8937...</td>\n",
       "      <td>F0FD783189F55BAFC331AD347EAE6863</td>\n",
       "      <td>160A90377E54124D0BD31DB6735F0B33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sale  SalesAmountInEuro  time_delay_for_conversion      click_timestamp  \\\n",
       "0     0               -1.0                         -1  2020-08-04 04:07:56   \n",
       "1     0               -1.0                         -1  2020-08-04 01:47:40   \n",
       "2     0               -1.0                         -1  2020-08-04 16:54:31   \n",
       "3     0               -1.0                         -1  2020-08-03 20:34:28   \n",
       "4     0               -1.0                         -1  2020-08-03 20:54:17   \n",
       "\n",
       "   nb_clicks_1week  product_price                 product_age_group  \\\n",
       "0               -1            0.0                                -1   \n",
       "1               -1            0.0                                -1   \n",
       "2               -1            0.0  4C90FD52FC53D2C1C205844CB69575AB   \n",
       "3               -1            0.0                                -1   \n",
       "4             1064            0.0  4C90FD52FC53D2C1C205844CB69575AB   \n",
       "\n",
       "                        device_type audience_id  \\\n",
       "0  7E56C27BFF0305E788DA55A029EC4988          -1   \n",
       "1  7E56C27BFF0305E788DA55A029EC4988          -1   \n",
       "2  FF2C446555E3822B0E0FC3406116E86D          -1   \n",
       "3  D7D1FB49049702BF6338894757E0D959          -1   \n",
       "4  7E56C27BFF0305E788DA55A029EC4988          -1   \n",
       "\n",
       "                     product_gender  ...               product_category(3)  \\\n",
       "0                                -1  ...                                -1   \n",
       "1                                -1  ...                                -1   \n",
       "2  C45A9AC6D102ACAEEDF0D6F78636D84A  ...                                -1   \n",
       "3                                -1  ...                                -1   \n",
       "4  1B491180398E2F0390E6A588B3BCE291  ...  1E629AECC2FB9BEF43331CBE8F2D7C08   \n",
       "\n",
       "                product_category(4) product_category(5) product_category(6)  \\\n",
       "0                                -1                  -1                  -1   \n",
       "1                                -1                  -1                  -1   \n",
       "2                                -1                  -1                  -1   \n",
       "3                                -1                  -1                  -1   \n",
       "4  341C9BD18A3277E6B104CAFC177DE796                  -1                  -1   \n",
       "\n",
       "  product_category(7)                   product_country  \\\n",
       "0                  -1  57A1D462A03BD076E029CF9310C11FC5   \n",
       "1                  -1  57A1D462A03BD076E029CF9310C11FC5   \n",
       "2                  -1  2AC62132FBCFA093B9426894A4BC6278   \n",
       "3                  -1  57A1D462A03BD076E029CF9310C11FC5   \n",
       "4                  -1  989EEF92F2A525DD896557425EA7C4C7   \n",
       "\n",
       "                         product_id  \\\n",
       "0  B69E439E41E0BEAD764ABF16D7FD96C9   \n",
       "1  404D3D9D03297504F3509032DCFA02F0   \n",
       "2  B09E51338E0EED59C5A859B13631C370   \n",
       "3  080614393A57816D7A655695E2DBE728   \n",
       "4  350271BAFD7C7AAF6FB1424CF3DD4827   \n",
       "\n",
       "                                       product_title  \\\n",
       "0                                                 -1   \n",
       "1                                                 -1   \n",
       "2  3DF2BEDE6A8FDFA7F97B97FFF6EF38CC 516EE9C34B839...   \n",
       "3                                                 -1   \n",
       "4  097C90F8BF5398AC7C486804F0801DEE E09E084DB8937...   \n",
       "\n",
       "                         partner_id                           user_id  \n",
       "0  E3DDEB04F8AFF944B11943BB57D2F620  5E2C678F6586B67F61A377E1534E01FC  \n",
       "1  E3DDEB04F8AFF944B11943BB57D2F620  E8247702C0DD294E0AE6B5B5E2F9E810  \n",
       "2  12E43E51784BDE3CB9E0EF6310A7D5C5  9FD001258907F541D497040C64383696  \n",
       "3  E3DDEB04F8AFF944B11943BB57D2F620  225741ACF2DBB2E5948268F5D5D352E6  \n",
       "4  F0FD783189F55BAFC331AD347EAE6863  160A90377E54124D0BD31DB6735F0B33  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'train_dataset.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values with standard value\n",
    "\n",
    "Fill missing values (based on project doc) with ```np.nan``` for numerical column and\n",
    "```pd.NA``` for categorical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_numerical_categorical_columns(data):\n",
    "    numerical_columns_df = data.select_dtypes(include=np.number)\n",
    "    numerical_column_names = numerical_columns_df.columns.tolist()\n",
    "\n",
    "    column_types_df = pd.DataFrame(data.columns, columns=['column name'])\n",
    "\n",
    "    column_types_df['numerical or categorical'] = np.where(column_types_df['column name'].isin(numerical_column_names),\n",
    "                                                           'numerical', 'categorical')\n",
    "\n",
    "    num_columns = column_types_df[column_types_df['numerical or categorical'] == 'numerical']['column name']\n",
    "    cat_columns = column_types_df[column_types_df['numerical or categorical'] == 'categorical']['column name']\n",
    "\n",
    "    return num_columns, cat_columns\n",
    "\n",
    "\n",
    "numerical_columns, categorical_columns = get_numerical_categorical_columns(df)\n",
    "\n",
    "df[numerical_columns] = df[numerical_columns].apply(lambda col: col.replace({-1: np.nan}))\n",
    "\n",
    "df['product_price'] = df['product_price'].replace({0: np.nan})\n",
    "\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda col: col.replace({'-1': np.nan}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Statistical Insight\n",
    "\n",
    "Find numerical and categorical columns and calculate statistics like:\n",
    "\n",
    "- Mean, Std, Quantiles, Number(And Percentage) Of Missing Values and etc for each column\n",
    "- Number of unique values and count number of values per each unique value for each categorical column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_columns_statistical_info(numerical_columns_df):\n",
    "    return numerical_columns_df.describe().append(\n",
    "        pd.Series(numerical_columns_df.isnull().sum(), name='n of missing values')\n",
    "    ).append(\n",
    "        pd.Series(numerical_columns_df.isnull().sum() / len(numerical_columns_df * 100), name='p of missing values')\n",
    "    )\n",
    "\n",
    "\n",
    "def categorical_columns_statistical_info(categorical_columns_df):\n",
    "    values_count_df: pd.DataFrame = categorical_columns_df.apply(pd.Series.value_counts)\n",
    "    values_count_df = values_count_df.fillna('-')\n",
    "\n",
    "    values_count_df.loc['n of not exist values'] = categorical_columns_df.isnull().sum()\n",
    "    values_count_df.loc['p of not exist values'] = categorical_columns_df.isnull().sum() / len(categorical_columns_df) * 100\n",
    "    values_count_df.loc['n of unique values'] = categorical_columns_df.nunique()\n",
    "\n",
    "    return values_count_df\n",
    "\n",
    "\n",
    "numerical_columns_stats = numerical_columns_statistical_info(df[numerical_columns.tolist()])\n",
    "categorical_columns_stats = categorical_columns_statistical_info(df[categorical_columns.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sale</th>\n",
       "      <th>SalesAmountInEuro</th>\n",
       "      <th>time_delay_for_conversion</th>\n",
       "      <th>nb_clicks_1week</th>\n",
       "      <th>product_price</th>\n",
       "      <th>product_category(7)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>13661.000000</td>\n",
       "      <td>1.360600e+04</td>\n",
       "      <td>53940.000000</td>\n",
       "      <td>12784.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.136610</td>\n",
       "      <td>117.030507</td>\n",
       "      <td>3.213666e+05</td>\n",
       "      <td>439.389006</td>\n",
       "      <td>85.491137</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.343437</td>\n",
       "      <td>383.010444</td>\n",
       "      <td>5.884293e+05</td>\n",
       "      <td>1541.251393</td>\n",
       "      <td>165.115302</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.023790</td>\n",
       "      <td>6.970000e+02</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.870000</td>\n",
       "      <td>4.277500e+03</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.565000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>124.990000</td>\n",
       "      <td>3.484675e+05</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>87.420000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>23691.224980</td>\n",
       "      <td>2.554631e+06</td>\n",
       "      <td>25390.000000</td>\n",
       "      <td>3928.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n of missing values</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>86339.000000</td>\n",
       "      <td>8.639400e+04</td>\n",
       "      <td>46060.000000</td>\n",
       "      <td>87216.000000</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p of missing values</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.863390</td>\n",
       "      <td>8.639400e-01</td>\n",
       "      <td>0.460600</td>\n",
       "      <td>0.872160</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Sale  SalesAmountInEuro  \\\n",
       "count                100000.000000       13661.000000   \n",
       "mean                      0.136610         117.030507   \n",
       "std                       0.343437         383.010444   \n",
       "min                       0.000000           0.000000   \n",
       "25%                       0.000000          23.023790   \n",
       "50%                       0.000000          51.870000   \n",
       "75%                       0.000000         124.990000   \n",
       "max                       1.000000       23691.224980   \n",
       "n of missing values       0.000000       86339.000000   \n",
       "p of missing values       0.000000           0.863390   \n",
       "\n",
       "                     time_delay_for_conversion  nb_clicks_1week  \\\n",
       "count                             1.360600e+04     53940.000000   \n",
       "mean                              3.213666e+05       439.389006   \n",
       "std                               5.884293e+05      1541.251393   \n",
       "min                               8.000000e+00         0.000000   \n",
       "25%                               6.970000e+02         6.000000   \n",
       "50%                               4.277500e+03        39.000000   \n",
       "75%                               3.484675e+05       198.000000   \n",
       "max                               2.554631e+06     25390.000000   \n",
       "n of missing values               8.639400e+04     46060.000000   \n",
       "p of missing values               8.639400e-01         0.460600   \n",
       "\n",
       "                     product_price  product_category(7)  \n",
       "count                 12784.000000                  0.0  \n",
       "mean                     85.491137                  NaN  \n",
       "std                     165.115302                  NaN  \n",
       "min                       0.180000                  NaN  \n",
       "25%                      15.830000                  NaN  \n",
       "50%                      35.565000                  NaN  \n",
       "75%                      87.420000                  NaN  \n",
       "max                    3928.000000                  NaN  \n",
       "n of missing values   87216.000000             100000.0  \n",
       "p of missing values       0.872160                  1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_columns_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we plot haetmap of correlation between numerical columns to see how correlated these features are."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = df[numerical_columns.tolist()].corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you see in the above heatmap, *product_price* has correlation about 0.3 with *nb_clicks_1week* and *SalesAmountInEuro* that is for now, the biggest correlation between two distinct features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Histogram for numerical columns would give us more information about how distributed the values of these columns are"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[numerical_columns.tolist()].hist(bins=8, figsize=(10, 10));"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you see in above histograms, most of data in each numerical column is null value, which for these columns *0* is symbol of null data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also it'll be good that if we plot these numerical columns' pairplot to get a better insight about these kind of columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.pairplot(df[numerical_columns.tolist()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "what we've just seen is only too much null data and sparse dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>product_age_group</th>\n",
       "      <th>device_type</th>\n",
       "      <th>audience_id</th>\n",
       "      <th>product_gender</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_category(1)</th>\n",
       "      <th>product_category(2)</th>\n",
       "      <th>product_category(3)</th>\n",
       "      <th>product_category(4)</th>\n",
       "      <th>product_category(5)</th>\n",
       "      <th>product_category(6)</th>\n",
       "      <th>product_country</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>partner_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0000FF7BF953B3D7281341F4C7B98E56</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00011AB400BCC95F3CCAF0779669E61C</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000158282ACE81B560694CBC1AA8EF8E</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>00017F6C780F948ED3D61F8312618978</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000190215E00C3D5AF4E421CB2468C65</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFF8E415120D1B5424786E1085D12BD</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FFFFE3EC68D8E06C3CE530F84D2E30FF</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n of not exist values</th>\n",
       "      <td>0</td>\n",
       "      <td>75603</td>\n",
       "      <td>39</td>\n",
       "      <td>71793</td>\n",
       "      <td>75554</td>\n",
       "      <td>65800</td>\n",
       "      <td>45502</td>\n",
       "      <td>45517</td>\n",
       "      <td>52316</td>\n",
       "      <td>71222</td>\n",
       "      <td>92009</td>\n",
       "      <td>98832</td>\n",
       "      <td>23754</td>\n",
       "      <td>23714</td>\n",
       "      <td>45763</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p of not exist values</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.603</td>\n",
       "      <td>0.039</td>\n",
       "      <td>71.793</td>\n",
       "      <td>75.554</td>\n",
       "      <td>65.8</td>\n",
       "      <td>45.502</td>\n",
       "      <td>45.517</td>\n",
       "      <td>52.316</td>\n",
       "      <td>71.222</td>\n",
       "      <td>92.009</td>\n",
       "      <td>98.832</td>\n",
       "      <td>23.754</td>\n",
       "      <td>23.714</td>\n",
       "      <td>45.763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n of unique values</th>\n",
       "      <td>67045</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3181</td>\n",
       "      <td>10</td>\n",
       "      <td>4769</td>\n",
       "      <td>21</td>\n",
       "      <td>144</td>\n",
       "      <td>698</td>\n",
       "      <td>909</td>\n",
       "      <td>441</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>45446</td>\n",
       "      <td>27694</td>\n",
       "      <td>183</td>\n",
       "      <td>96766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246979 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 click_timestamp product_age_group  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56               -                 -   \n",
       "00011AB400BCC95F3CCAF0779669E61C               -                 -   \n",
       "000158282ACE81B560694CBC1AA8EF8E               -                 -   \n",
       "00017F6C780F948ED3D61F8312618978               -                 -   \n",
       "000190215E00C3D5AF4E421CB2468C65               -                 -   \n",
       "...                                          ...               ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD               -                 -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF               -                 -   \n",
       "n of not exist values                          0             75603   \n",
       "p of not exist values                        0.0            75.603   \n",
       "n of unique values                         67045                 8   \n",
       "\n",
       "                                 device_type audience_id product_gender  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56           -           -              -   \n",
       "00011AB400BCC95F3CCAF0779669E61C           -           -              -   \n",
       "000158282ACE81B560694CBC1AA8EF8E           -           -              -   \n",
       "00017F6C780F948ED3D61F8312618978           -           -              -   \n",
       "000190215E00C3D5AF4E421CB2468C65           -           -              -   \n",
       "...                                      ...         ...            ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD           -           -              -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF           -           -              -   \n",
       "n of not exist values                     39       71793          75554   \n",
       "p of not exist values                  0.039      71.793         75.554   \n",
       "n of unique values                         3        3181             10   \n",
       "\n",
       "                                 product_brand product_category(1)  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56             -                   -   \n",
       "00011AB400BCC95F3CCAF0779669E61C             -                   -   \n",
       "000158282ACE81B560694CBC1AA8EF8E             -                   -   \n",
       "00017F6C780F948ED3D61F8312618978             -                   -   \n",
       "000190215E00C3D5AF4E421CB2468C65             -                   -   \n",
       "...                                        ...                 ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD             -                   -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF             -                   -   \n",
       "n of not exist values                    65800               45502   \n",
       "p of not exist values                     65.8              45.502   \n",
       "n of unique values                        4769                  21   \n",
       "\n",
       "                                 product_category(2) product_category(3)  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56                   -                   -   \n",
       "00011AB400BCC95F3CCAF0779669E61C                   -                   -   \n",
       "000158282ACE81B560694CBC1AA8EF8E                   -                   -   \n",
       "00017F6C780F948ED3D61F8312618978                   -                   -   \n",
       "000190215E00C3D5AF4E421CB2468C65                   -                   -   \n",
       "...                                              ...                 ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD                   -                   -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF                   -                   -   \n",
       "n of not exist values                          45517               52316   \n",
       "p of not exist values                         45.517              52.316   \n",
       "n of unique values                               144                 698   \n",
       "\n",
       "                                 product_category(4) product_category(5)  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56                   -                   -   \n",
       "00011AB400BCC95F3CCAF0779669E61C                   -                   -   \n",
       "000158282ACE81B560694CBC1AA8EF8E                   -                   -   \n",
       "00017F6C780F948ED3D61F8312618978                   -                   -   \n",
       "000190215E00C3D5AF4E421CB2468C65                   -                   -   \n",
       "...                                              ...                 ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD                   -                   -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF                   -                   -   \n",
       "n of not exist values                          71222               92009   \n",
       "p of not exist values                         71.222              92.009   \n",
       "n of unique values                               909                 441   \n",
       "\n",
       "                                 product_category(6) product_country  \\\n",
       "0000FF7BF953B3D7281341F4C7B98E56                   -               -   \n",
       "00011AB400BCC95F3CCAF0779669E61C                   -               -   \n",
       "000158282ACE81B560694CBC1AA8EF8E                   -               -   \n",
       "00017F6C780F948ED3D61F8312618978                   -               -   \n",
       "000190215E00C3D5AF4E421CB2468C65                   -               -   \n",
       "...                                              ...             ...   \n",
       "FFFF8E415120D1B5424786E1085D12BD                   -               -   \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF                   -               -   \n",
       "n of not exist values                          98832           23754   \n",
       "p of not exist values                         98.832          23.754   \n",
       "n of unique values                                88              16   \n",
       "\n",
       "                                 product_id product_title partner_id user_id  \n",
       "0000FF7BF953B3D7281341F4C7B98E56        1.0             -          -       -  \n",
       "00011AB400BCC95F3CCAF0779669E61C        1.0             -          -       -  \n",
       "000158282ACE81B560694CBC1AA8EF8E        1.0             -          -       -  \n",
       "00017F6C780F948ED3D61F8312618978          -             -          -     1.0  \n",
       "000190215E00C3D5AF4E421CB2468C65          -             -          -     1.0  \n",
       "...                                     ...           ...        ...     ...  \n",
       "FFFF8E415120D1B5424786E1085D12BD          -             -          -     1.0  \n",
       "FFFFE3EC68D8E06C3CE530F84D2E30FF          -             -          -     1.0  \n",
       "n of not exist values                 23714         45763          0       0  \n",
       "p of not exist values                23.714        45.763        0.0     0.0  \n",
       "n of unique values                    45446         27694        183   96766  \n",
       "\n",
       "[246979 rows x 17 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Categorical features, we draw barplot for different values these columns have, and as you saw before, some categorical columns have so many distinct values hence we consider these barplots only for 10 values which have maximum counts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "fig, axs = plt.subplots(17, figsize=(50, 400))\n",
    "for index, column in enumerate(tqdm(categorical_columns.tolist())):\n",
    "    axs[index].set_title(column)\n",
    "    axs[index].bar(df[column].value_counts().index[:10], df[column].value_counts()[:10])\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns are completely marked from target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.count_nonzero((df['Sale'] == 1) & (df['SalesAmountInEuro'] != -1)) / np.count_nonzero(df['Sale'] == 1))\n",
    "print(np.count_nonzero((df['Sale'] == 1) & (df['time_delay_for_conversion'] != -1)) / np.count_nonzero(df['Sale'] == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns\n",
    "\n",
    "Description of the columns that will be dropped:\n",
    "- SalesAmountInEuro: This column is completely marked\n",
    "from column Sale. so i drop this column. (86% missing)\n",
    "- time_delay_for_conversion: This column is almost\n",
    "completely marked from column Sale. so i also drop this column. (86% missing)\n",
    "- click_timestamp: i extract day and hour from this column and then drop it.\n",
    "- product_category(7): (100% missing)\n",
    "- audience_id: doc didnt explain this column.\n",
    "- product_title: this column has ~27k unique value while dataset size has 100k rows. so\n",
    "this column isnt useful.\n",
    "- user_id: this column has ~97k unique value while dataset size has 100k rows. so\n",
    "this column isnt useful.\n",
    "- product_id: this column has ~45k unique value while dataset size has 100k rows. so\n",
    "this column isnt useful.\n",
    "\n",
    "> Sale: this is target column and will be appended to dataset after eda and feature engineering done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "target_column_name = 'Sale'\n",
    "target = df[target_column_name]\n",
    "\n",
    "# df['click_timestamp'] = pd.to_datetime(df['click_timestamp'])\n",
    "# df['day'] = df['click_timestamp'].dt.day\n",
    "# df['hour'] = df['click_timestamp'].dt.hour\n",
    "\n",
    "df.drop(\n",
    "    [\n",
    "        target_column_name,\n",
    "        'product_category(7)',\n",
    "        'SalesAmountInEuro',\n",
    "        'time_delay_for_conversion',\n",
    "        'audience_id',\n",
    "        'click_timestamp',\n",
    "        'product_title',\n",
    "        'user_id',\n",
    "        'product_id',\n",
    "    ],\n",
    "    axis=1, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Drop Rows have less than ```threshold``` non-missing values in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "df = df.dropna(thresh=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values\n",
    "\n",
    "Fill categorical columns missing values with column median and numerical columns with column mode (most frequent value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numerical_columns, categorical_columns = get_numerical_categorical_columns(df)\n",
    "\n",
    "for column in numerical_columns:\n",
    "    df.loc[df[column].isnull(), column] = df.loc[~df[column].isnull(), column].median()\n",
    "\n",
    "for column in categorical_columns:\n",
    "    df.loc[df[column].isnull(), column] = df.loc[~df[column].isnull(), column].mode().iat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I also used ```IterativeImputer``` from ```sklearn``` to\n",
    "fill missing values but its not better than basic last strategy.\n",
    "\n",
    "```\n",
    "to_impute_columns = categorical_columns\n",
    "\n",
    "initial_strategy = 'most_frequent'\n",
    "# estimator = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "estimator = KNeighborsClassifier(n_neighbors=9)\n",
    "imputer: IterativeImputer = IterativeImputer(random_state=0,\n",
    "                                             estimator=estimator,\n",
    "                                             initial_strategy=initial_strategy,\n",
    "                                             max_iter=10,\n",
    "                                             verbose=2)\n",
    "\n",
    "imputed = imputer.fit_transform(data_frame[to_impute_columns])\n",
    "data_frame.loc[:, to_impute_columns] = imputed\n",
    "\n",
    "# Numeric Columns\n",
    "to_impute_columns = to_impute_columns.append(numerical_columns)\n",
    "\n",
    "initial_strategy = 'median'\n",
    "estimator = KNeighborsRegressor(n_neighbors=9)\n",
    "imputer: IterativeImputer = IterativeImputer(random_state=0,\n",
    "                                             estimator=estimator,\n",
    "                                             initial_strategy=initial_strategy,\n",
    "                                             max_iter=10,\n",
    "                                             verbose=2)\n",
    "\n",
    "imputed = imputer.fit_transform(data_frame[to_impute_columns])\n",
    "data_frame.loc[:, to_impute_columns] = imputed\n",
    "```\n",
    "\n",
    "> [Handle Missing Values](https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Columns\n",
    "\n",
    "I encode categorical columns with ```LabelEncoder``` class of ```\n",
    "sklearn``` lib. i dont use the One-Hot strategy because the number of unique values in some columns is too large.\n",
    "label encoder strategy works in this regard and greatly reduces the amount of data.\n",
    "\n",
    "> [Encode Categorical Columns](https://machinelearningmastery.com/one-hot-encoding-for-categorical-data/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Categorical Columns Encode\n",
    "for c_column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    encoded: pd.Series = le.fit_transform(df.loc[~df[c_column].isnull(), c_column])\n",
    "    df.loc[~df[c_column].isnull(), c_column] = encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also used ```OneHotEncoder``` from ```sklearn``` lib\n",
    " but the result was no different from the one mentioned above.\n",
    " In addition, this method will require less memory to save.\n",
    "\n",
    "```\n",
    "enc = OneHotEncoder()\n",
    "enc_df = pd.DataFrame(enc.fit_transform(data_frame[categorical_columns]).toarray())\n",
    "data_frame.drop(categorical_columns, axis=1, inplace=True)\n",
    "data_frame = data_frame.join(enc_df)\n",
    "```\n",
    "\n",
    "### Data Normalization\n",
    "\n",
    "I normalized the numerical columns (not categorical columns!) with the help of ```StandardScaler``` class of\n",
    "```sklearn``` lib.\n",
    " ```StandardScaler``` applies the following conversion to\n",
    " the data:\n",
    "### <center>$Tx = \\frac{x - \\mu}{\\sigma}$</center>\n",
    "\n",
    "> [Data Normalization](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalizer = StandardScaler()\n",
    "# normalizer = MinMaxScaler()\n",
    "# df.loc[:, numerical_columns] = normalizer.fit_transform(df[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append Target Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df[target_column_name] = target\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "data_frame = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Modified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical_columns len: 2\n",
      "numerical_columns: ['nb_clicks_1week', 'product_price']\n",
      "categorical_columns len: 12\n",
      "categorical_columns: ['product_age_group', 'device_type', 'product_gender', 'product_brand', 'product_category(1)', 'product_category(2)', 'product_category(3)', 'product_category(4)', 'product_category(5)', 'product_category(6)', 'product_country', 'partner_id']\n",
      "dataset dimensions: {data_frame.shape[0]}\n"
     ]
    }
   ],
   "source": [
    "print(f'numerical_columns len: {len(numerical_columns)}')\n",
    "print(f'numerical_columns: {numerical_columns.tolist()}')\n",
    "\n",
    "print(f'categorical_columns len: {len(categorical_columns)}')\n",
    "print(f'categorical_columns: {categorical_columns.tolist()}')\n",
    "\n",
    "print('dataset dimensions: {data_frame.shape[0]}')\n",
    "pd.DataFrame.to_csv(data_frame,\n",
    "                    'modified_dataset.csv',\n",
    "                    sep='\\t',\n",
    "                    index=False,\n",
    "                    header=False\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "***\n",
    "# Model\n",
    "\n",
    "I have taught three models for this project.\n",
    "These three models are as follows:\n",
    "\n",
    "- Extreme Deep Factorization Model\n",
    "> [Extreme Deep Factorization Model](https://towardsdatascience.com/extreme-deep-factorization-machine-xdeepfm-1ba180a6de78)\n",
    "\n",
    "- Wide And Deep Model\n",
    "> [Wide And Deep Model](https://medium.com/analytics-vidhya/wide-deep-learning-for-recommender-systems-dc99094fc291)\n",
    "\n",
    "- XGBoost Model\n",
    "> [XGBoost Model](https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The first two models use the ```pytorch-fm``` lib and\n",
    "the third model uses the ```xgb``` library,\n",
    "which is installed with the help of the following commands."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install torchfm\n",
    "pip install xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Deep Factorization Model (xDeepFM)\n",
    "\n",
    "Each model is described in detail in the following sections:\n",
    "\n",
    "### Hybrid Approach (DNN + FM)\n",
    "\n",
    "The current recommendation landscape is\n",
    "dominated by FM/DNN based models. But\n",
    "some good hybrid architectures which fuse FM\n",
    "and DNN based systems are also coming up.\n",
    "\n",
    "1. Factorization Machine (FM) based approach\n",
    "\n",
    "    - Learns pattern on combinatorial features automatically\n",
    "    - Generalize well to unseen features\n",
    "    - Tries to capture all feature interactions which results in learning of useless interactions. This might introduce noise.\n",
    "\n",
    "2. Deep Neural Network (DNN) based approach\n",
    "\n",
    "    - Learns sophisticated and selective feature interactions\n",
    "    - Feature interactions are modeled at an elemental level. One hot encoding is used for categorical variables to represent them in dimension D. This will be fed into a fully connected layer. This is in stark contrast to FM based approaches which models feature interactions at a vector level (User vector * Item Vector).\n",
    "\n",
    "3. Hybrid approach (DNN + FM)\n",
    "\n",
    "    - Learns both low/high order feature interactions which can capture both memorization and generalization.\n",
    "\n",
    "    > Memorisation can be loosely defined as learning the frequent co-occurrence of items or features and exploiting the correlation available in the historical data.\n",
    "    > 1. Memorisation of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalisation requires more feature engineering effort.\n",
    "    > 2. Recommendations based on memorisation are usually more topical and directly relevant to the items on which users have already performed actions.\n",
    "    > 3. Memorisation can be achieved effectively using cross-product transformations over sparse features. This explains how the co-occurrence of a feature pair correlates with the target label.\n",
    "    > 4. One limitation of cross-product transformations is that they do not generalise to query-item feature pairs that have not appeared in the training data.\n",
    "    > 5. Wide linear models can effectively memorise sparse feature interactions using cross-product feature transformations.\n",
    "    >  Generalisation, on the other hand, is based on transitivity of correlation and explores new feature combinations that have never or rarely occurred in the past.\n",
    "    > 1. With less feature engineering, deep neural networks can generalise better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features.\n",
    "    > 2. However, deep neural networks with embeddings can over-generalise and recommend less relevant items when the user-item interactions are sparse and high-rank.\n",
    "    > 3. Generalisation tends to improve the diversity of the recommended items. Generalisation can be added by using features that are less granular , but manual feature engineering is often required.\n",
    "    > 4. For massive-scale online recommendation and ranking systems in an industrial setting, generalised linear models such as logistic regression are widely used because they are simple, scalable and interpretable. The models are often trained on binarised sparse features with one-hot encoding.\n",
    "\n",
    "    > [Generalization And Memorization](https://medium.com/analytics-vidhya/memorization-and-deep-neural-networks-5b56aa9f94b8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Extreme Deep Factorization Machine\n",
    "\n",
    "#### Model Architectures\n",
    "\n",
    "![xDeepFM Model Architectures](figures/xdfm.png)\n",
    "We omit the description of\n",
    "the FM component for\n",
    "brevity.\n",
    "\n",
    "> [Factorization Machine](https://d2l.ai/chapter_recommender-systems/fm.html)\n",
    "\n",
    "xDeepFM comprises of 3 parts:\n",
    "1. The linear model ( Directly work on top of raw input features )\n",
    "2. Plain DNN (Works on top of dense feature embeddings)\n",
    "3. Introducing Compressed Interaction Network (CNN) (Works on top of dense feature embeddings)\n",
    "Out of these 3, CIN is unique to xDeepFM.\n",
    "\n",
    "CIN characteristics:\n",
    "\n",
    "CIN is inducted by xDeepFM due to the following benefits:\n",
    "- It learns feature interactions at a vector wise level, not at a bitwise level.\n",
    "- It measures high order feature interactions explicitly.\n",
    "- Its complexity does not grow exponentially with the degree of interactions.\n",
    "\n",
    "> [Compressed Interaction Network](https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883)\n",
    "\n",
    "\n",
    "Linear, CIN, and DNN are all trained parallelly\n",
    "\n",
    "Linear model, plain DNN, and CIN\n",
    "are trained in parallel, and the output of\n",
    "the model is to apply the sigmoid function to the linear output of these three models.\n",
    "\n",
    "#### Trained Model Architectures\n",
    "\n",
    "##### TorchViz Result:\n",
    "![Extreme Deep Factorization Machine](deep_model_archits/Extreme%20Deep%20Factorization%20Model%20tz%20result.png)\n",
    "\n",
    "##### Hidden Layer Result:\n",
    "![Extreme Deep Factorization Machine](deep_model_archits/Extreme%20Deep%20Factorization%20Model%20hl%20result.png)\n",
    "\n",
    "#### Implement\n",
    "\n",
    "xDeepFM model has been implemented in the DeepModel.py (set DEEP_MODEL_NAME parameter to XDFM_STR constant) and\n",
    "With the help of following command, xDeepFM model is trained:\n",
    "\n",
    "> The code has been commented."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python DeepModel.py # DEEP_MODEL_NAME parameter set to XDFM_STR constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning\n",
    "\n",
    "Model hyper-parameters tuning is\n",
    " done with the ```optuna```\n",
    " library.\n",
    "\n",
    "The BayesianSearch algorithm used in this\n",
    "section has better performance than GridSearch and RandomSearch algorithm.\n",
    "\n",
    "Bayesian optimization methods are efficient because they select hyperparameters in an informed manner. By prioritizing hyperparameters that appear more promising from past results, Bayesian methods can find the best hyperparameters in lesser time (in fewer iterations) than both grid search and random search.\n",
    "\n",
    "This library\n",
    "\n",
    "> [Grid search, Random search, Bayesian optimization](https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1#:~:text=Bayesian%20optimization%20methods%20are%20efficient,grid%20search%20and%20random%20search.)\n",
    "\n",
    "```\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "\n",
    "study.optimize(objective_function, n_trials=15)\n",
    "\n",
    "def objective_function(trial: optuna.Trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_discrete_uniform('learning_rate', 1e-4, 6 * 1e-4, q=1 * 1e-4),\n",
    "        'eps': trial.suggest_categorical('eps', [1e-8, ]),\n",
    "        'weight_decay': trial.suggest_discrete_uniform('weight_decay', 1 * 1e-6, 5 * 1e-6, q=1 * 1e-6),\n",
    "        'dropout': trial.suggest_discrete_uniform('dropout', 0.15, 0.35, q=0.1),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [512]),\n",
    "        'amsgrad': trial.suggest_categorical('amsgrad', [False, True]),\n",
    "        'epochs': trial.suggest_categorical('epochs', [25]),\n",
    "    }\n",
    "    # Code logic\n",
    "```\n",
    "\n",
    "#### Result\n",
    "\n",
    "With the help of ```MLFlow```\n",
    " Library, viewing\n",
    "the results is very simple. To view\n",
    "the results, just run the following command in\n",
    "the terminal. (In extreme deep factorization machine model section.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mlflow server -p 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the results, the following link should be opened in the browser:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "127.0.0.1:5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics and Parameters:\n",
    "\n",
    "Metrics are used in the evaluation is:\n",
    "\n",
    "- F-Score\n",
    "- Accuracy\n",
    "- AUC (Area Under Curve)\n",
    "- Binary Cross Entropy Loss\n",
    "\n",
    "\n",
    "![Metrics and Parameters](results_image/xdfm_metrics_result.png)\n",
    "\n",
    "A good fit is the goal of the learning algorithm and exists between an overfit and underfit model.\n",
    "\n",
    "A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n",
    "\n",
    "The loss of the model will almost always be lower on the training dataset than the validation dataset. This means that we should expect some gap between the train and validation loss learning curves. This gap is referred to as the “generalization gap.”\n",
    "\n",
    "A plot of learning curves shows a good fit if:\n",
    "\n",
    "- The plot of training loss decreases to a point of stability.\n",
    "- The plot of validation loss decreases to a point of stability and has a small gap with the training loss.\n",
    "Continued training of a good fit will likely lead to an overfit.\n",
    "\n",
    "##### Loss And F1-Score in epochs (Tunned Model):\n",
    "![Loss And F1-Score](results_image/xdfm_perepoch.png)\n",
    "\n",
    "Learning curves (LCs) are deemed effective tools for monitoring the performance of workers exposed to a new task. LCs provide a mathematical representation of the learning process that takes place as task repetition occurs.\n",
    "\n",
    "##### Comparing Runs:\n",
    "![Comparing Runs](results_image/xdfm_comaring_models.png)\n",
    "\n",
    "##### Hyperparameters Tuning Parallel Coordinates Plot:\n",
    "![Hyperparameters Tuning Parallel Coordinates Plot](results_image/xdfm_p_plot.png)\n",
    "\n",
    "***\n",
    "\n",
    "## Wide & Deep Model\n",
    "\n",
    "#### Model Architectures\n",
    "\n",
    "![Wide & Deep Model Architectures](figures/w&dm.png)\n",
    "\n",
    "In generalisation scenario Embedding-based models, such as factorization machines or deep neural networks, can generalize to previously unseen query-item feature pairs by learning a low-dimensional dense embedding vector for each query and item feature, with less burden of feature engineering. However, it is difficult to learn effective low-dimensional representations for queries and items when the underlying query-item matrix is sparse and high-rank, such as users with specific preferences or niche items with a narrow appeal.\n",
    "In such cases, there should be no interactions between most query-item pairs, but dense embeddings will lead to nonzero predictions for all query-item pairs, and thus can over-generalize and make less relevant recommendations.\n",
    "In memorisation scenario\n",
    "On the other hand, linear models with cross-product feature transformations can memorize these “exception rules” with much fewer parameters.\n",
    "deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings.\n",
    "\n",
    "1. **Wide Component**: The wide component is a generalized linear model\n",
    "    of the form $\\mathbf{y = w^t x + b}$, as illustrated\n",
    "    in above Figure (left).\n",
    "    y is the prediction, $\\mathbf{x = [x_1, x_2, …, x_d]}$ is a vector\n",
    "    of d features, $\\mathbf{w = [w_1, w_2, …, w_d]}$ are the model parameters\n",
    "    and b is the bias. The feature set includes raw input\n",
    "    features and transformed. One of the most important\n",
    "    transformations is the cross-product transformation,\n",
    "    which is defined as:\n",
    "\n",
    "    ### <center>$\\phi_k (x) = \\displaystyle{\\prod_{i=1}^{d}} \\; x_i^{c_{k_i}}$</center>\n",
    "\n",
    "2. **Deep Component**\n",
    "    The deep component is a feed-forward neural network,\n",
    "    as shown in Above Figure (right). For categorical features,\n",
    "    the original inputs are feature strings\n",
    "    (e.g., “language=en”). Each of these sparse,\n",
    "    high-dimensional categorical features are first converted\n",
    "    into a low-dimensional and dense real-valued vector,\n",
    "    often referred to as an embedding vector. The dimensionality\n",
    "    of the embeddings are usually on the order of O(10) to\n",
    "    O(100). The embedding vectors are initialized randomly and\n",
    "    then the values are trained to minimize the final loss\n",
    "    function during model training. These low-dimensional\n",
    "    dense embedding vectors are then fed into the hidden layers\n",
    "    of a neural network in the forward pass. Specifically,\n",
    "    each hidden layer performs the following computation:\n",
    "\n",
    "    ### <center>$a^{(l+1)} = f(W^{(l)}a^{(l)} + b^{(l)}) $</center>\n",
    "\n",
    "    where l is the layer number and $f$\n",
    "    is the activation function, often ReLUs.\n",
    "    $a^{(l)}, b^{(l)}, W^{(l)}$ are the activations, bias, and model weights at l-th layer.\n",
    "\n",
    "    deep neural networks can generalize to previously unseen feature interactions through low dimensional embeddings.\n",
    "\n",
    "    > [Wide And Deep Model Addition Source](https://medium.com/analytics-vidhya/wide-deep-learning-for-recommender-systems-dc99094fc291)\n",
    "\n",
    "#### Trained Model Architectures\n",
    "\n",
    "##### TorchViz Result:\n",
    "![Wide & Deep Model](deep_model_archits/Wide%20&%20Deep%20Model%20tz%20result.png)\n",
    "\n",
    "##### Hidden Layer Result:\n",
    "![Wide & Deep Model](deep_model_archits/Wide%20&%20Deep%20Model%20hl%20result.png)\n",
    "\n",
    "#### Implement\n",
    "\n",
    "Wide & Deep model has been implemented in the DeepModel.py (set DEEP_MODEL_NAME parameter to WADM_STR constant) and\n",
    "With the help of following command, xDeepFM model is trained:\n",
    "\n",
    "> The code has been commented."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python DeepModel.py # DEEP_MODEL_NAME parameter set to WADM_STR constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning\n",
    "\n",
    "Model hyper-parameters tuning is\n",
    " done with the ```optuna```\n",
    " library.\n",
    "\n",
    "The BayesianSearch algorithm used in this\n",
    "section has better performance than GridSearch and RandomSearch algorithm.\n",
    "\n",
    "Bayesian optimization methods are efficient because they select hyperparameters in an informed manner. By prioritizing hyperparameters that appear more promising from past results, Bayesian methods can find the best hyperparameters in lesser time (in fewer iterations) than both grid search and random search.\n",
    "\n",
    "This library\n",
    "\n",
    "> [Grid search, Random search, Bayesian optimization](https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1#:~:text=Bayesian%20optimization%20methods%20are%20efficient,grid%20search%20and%20random%20search.)\n",
    "\n",
    "```\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "\n",
    "study.optimize(objective_function, n_trials=15)\n",
    "\n",
    "def objective_function(trial: optuna.Trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_discrete_uniform('learning_rate', 1e-4, 6 * 1e-4, q=1 * 1e-4),\n",
    "        'eps': trial.suggest_categorical('eps', [1e-8, ]),\n",
    "        'weight_decay': trial.suggest_discrete_uniform('weight_decay', 1 * 1e-6, 5 * 1e-6, q=1 * 1e-6),\n",
    "        'dropout': trial.suggest_discrete_uniform('dropout', 0.15, 0.35, q=0.1),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [512]),\n",
    "        'amsgrad': trial.suggest_categorical('amsgrad', [False, True]),\n",
    "        'epochs': trial.suggest_categorical('epochs', [35]),\n",
    "    }\n",
    "    # Code logic\n",
    "```\n",
    "\n",
    "#### Result\n",
    "\n",
    "With the help of ```MLFlow```\n",
    " Library, viewing\n",
    "the results is very simple. To view\n",
    "the results, just run the following command in\n",
    "the terminal. (In wide and deep model section.)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mlflow server -p 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the results, the following link should be opened in the browser:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "127.0.0.1:5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics and Parameters:\n",
    "\n",
    "\n",
    "Metrics are used in the evaluation is:\n",
    "\n",
    "- F-Score\n",
    "- Accuracy\n",
    "- AUC (Area Under Curve)\n",
    "- Binary Cross Entropy Loss\n",
    "\n",
    "\n",
    "![Metrics and Parameters](results_image/wadm_metrics_result.png)\n",
    "\n",
    "A good fit is the goal of the learning algorithm and exists between an overfit and underfit model.\n",
    "\n",
    "A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values.\n",
    "\n",
    "The loss of the model will almost always be lower on the training dataset than the validation dataset. This means that we should expect some gap between the train and validation loss learning curves. This gap is referred to as the “generalization gap.”\n",
    "\n",
    "A plot of learning curves shows a good fit if:\n",
    "\n",
    "- The plot of training loss decreases to a point of stability.\n",
    "- The plot of validation loss decreases to a point of stability and has a small gap with the training loss.\n",
    "Continued training of a good fit will likely lead to an overfit.\n",
    "\n",
    "##### Loss And F1-Score in epochs (Tunned Model):\n",
    "![Loss And F1-Score](results_image/wadm_perepoch.png)\n",
    "\n",
    "Learning curves (LCs) are deemed effective tools for monitoring the performance of workers exposed to a new task. LCs provide a mathematical representation of the learning process that takes place as task repetition occurs.\n",
    "\n",
    "\n",
    "##### Comparing Runs:\n",
    "![Comparing Runs](results_image/wadm_comparing_models.png)\n",
    "\n",
    "##### Hyperparameters Tuning Parallel Coordinates Plot:\n",
    "![Hyperparameters Tuning Parallel Coordinates Plot](results_image/wadm_p_plot.png)\n",
    "\n",
    "***\n",
    "\n",
    "## XGBoost Model\n",
    "\n",
    "![XGBoost Model](figures/xgb.jpeg)\n",
    "\n",
    "XGBoost is a decision-tree-based\n",
    "ensemble Machine Learning algorithm that\n",
    "uses a gradient boosting framework.\n",
    "In prediction problems involving\n",
    "unstructured data (images, text, etc.)\n",
    "artificial neural networks tend to\n",
    "outperform all other algorithms or\n",
    "frameworks. However, when it comes\n",
    "to small-to-medium structured/tabular\n",
    "data, decision tree based algorithms\n",
    "are considered best-in-class\n",
    "right now.\n",
    "\n",
    "![XGBoost Model](figures/xgb2.png)\n",
    "\n",
    "1. **Decision Tree**: Every hiring manager has a set of criteria such as education level, number of years of experience, interview performance. A decision tree is analogous to a hiring manager interviewing candidates based on his or her own criteria.\n",
    "2. **Bagging**: Now imagine instead of a single interviewer, now there is an interview panel where each interviewer has a vote. Bagging or bootstrap aggregating involves combining inputs from all interviewers for the final decision through a democratic voting process.\n",
    "3. **Random Forest**: It is a bagging-based algorithm with a key difference wherein only a subset of features is selected at random. In other words, every interviewer will only test the interviewee on certain randomly selected qualifications (e.g. a technical interview for testing programming skills and a behavioral interview for evaluating non-technical skills).\n",
    "4. **Boosting**: This is an alternative approach where each interviewer alters the evaluation criteria based on feedback from the previous interviewer. This ‘boosts’ the efficiency of the interview process by deploying a more dynamic evaluation process.\n",
    "5. **Gradient Boosting**: A special case of boosting where errors are minimized by gradient descent algorithm e.g. the strategy consulting firms leverage by using case interviews to weed out less qualified candidates.\n",
    "6. **XGBoost**: Think of XGBoost as gradient boosting on ‘steroids’ (well it is called ‘Extreme Gradient Boosting’ for a reason!). It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.\n",
    "\n",
    "#### Performance Comparison\n",
    "\n",
    "![XGBoost Performance Comparison](figures/xgb3.jpeg)\n",
    "\n",
    "As demonstrated in the chart above, XGBoost model has the best combination of prediction performance and processing time compared to other algorithms. Other rigorous benchmarking studies have produced similar results. No wonder XGBoost is widely used in recent Data Science competitions.\n",
    "\n",
    "> [XGBoost Algorithm](https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement\n",
    "\n",
    "XGBoost model has been implemented in the XGBModel.py and\n",
    "With the help of following command, XGBoost model is trained:\n",
    "\n",
    "> The code has been commented."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python XGBModel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters tuning\n",
    "\n",
    "Model hyper-parameters tuning is\n",
    " done with the ```scikit-optimize```\n",
    " library.\n",
    "\n",
    "The BayesianSeach algorithm used in this\n",
    "section has better performance than GridSearch and RandomSearch algorithm.\n",
    "\n",
    "Bayesian optimization methods are efficient because they select hyperparameters in an informed manner. By prioritizing hyperparameters that appear more promising from past results, Bayesian methods can find the best hyperparameters in lesser time (in fewer iterations) than both grid search and random search.\n",
    "\n",
    "This library\n",
    "\n",
    "> [Grid search, Random search, Bayesian optimization](https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1#:~:text=Bayesian%20optimization%20methods%20are%20efficient,grid%20search%20and%20random%20search.)\n",
    "\n",
    "```\n",
    "cross_validation_n_fold = 5\n",
    "opt = BayesSearchCV(\n",
    "    xgb.XGBClassifier(),\n",
    "    search_spaces={\n",
    "        'n_estimator': Integer(low=500, high=1500, prior='uniform'),\n",
    "        'max_depth': Integer(low=5, high=10, prior='uniform'),\n",
    "        'booster': Categorical(['dart', 'gbtree']),\n",
    "        'use_label_encoder': Categorical([False]),\n",
    "        'eval_metric': Categorical(['logloss']),\n",
    "    },\n",
    "    n_iter=50,\n",
    "    cv=cross_validation_n_fold,\n",
    "    scoring=lambda estimator, X, y: f1_score(y, estimator.predict(X)),\n",
    "    return_train_score=True,\n",
    "    verbose=0\n",
    ")\n",
    "```\n",
    "\n",
    "This library also supports Cross-Validation and\n",
    " has been used in this project.\n",
    "\n",
    "#### Result\n",
    "\n",
    "With the help of ```MLFlow```\n",
    " Library, viewing\n",
    "the results is very simple. To view\n",
    "the results, just run the following command in\n",
    "the terminal."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mlflow server -p 5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the results, the following link should be opened in the browser:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "127.0.0.1:5678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics and Parameters:\n",
    "\n",
    "Metrics are used in the evaluation is:\n",
    "\n",
    "- F-Score\n",
    "- Accuracy\n",
    "- AUC (Area Under Curve)\n",
    "- Log Loss\n",
    "\n",
    "![Metrics and Parameters](results_image/xgbmetrics_result.png)\n",
    "\n",
    "##### Comparing Runs:\n",
    "![Comparing Runs](results_image/xgb_comparing_models.png)\n",
    "\n",
    "##### Hyperparameters Tuning Parallel Coordinates Plot:\n",
    "![Hyperparameters Tuning Parallel Coordinates Plot](results_image/xgb_p_plot.png)\n",
    "\n",
    "\n",
    "## Containers Pipeline and Deployment\n",
    "\n",
    "For pipeline instructions please visit our github repository:"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "https://github.com/yasharzb/Criteo-recommender"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}